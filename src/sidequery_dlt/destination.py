"""
Iceberg REST Catalog destination for dlt.

This custom destination writes data to Apache Iceberg tables using a REST catalog
(Polaris, Unity Catalog, AWS Glue, Nessie, etc.).
"""

import time
import logging
from typing import Optional
from pathlib import Path

import dlt
import pyarrow.parquet as pq
from dlt.common.schema import TTableSchema
from pyiceberg.catalog import load_catalog
from pyiceberg.exceptions import (
    NoSuchTableError,
    CommitFailedException,
)

from .schema_converter import convert_dlt_to_iceberg_schema
from .partition_builder import build_partition_spec
from pyiceberg.io.pyarrow import schema_to_pyarrow

logger = logging.getLogger(__name__)


@dlt.destination(
    batch_size=0,  # Receive file paths instead of data items
    loader_file_format="parquet",
    name="iceberg_rest",
    naming_convention="snake_case",
    skip_dlt_columns_and_tables=True,  # Skip internal dlt tables
    max_parallel_load_jobs=5,
    loader_parallelism_strategy="table-sequential",
)
def iceberg_rest(
    items: str,  # File path when batch_size=0
    table: TTableSchema,
    # Catalog configuration
    catalog_uri: str = dlt.secrets.value,
    warehouse: Optional[str] = dlt.secrets.value,
    namespace: str = "default",
    # Authentication (OAuth2)
    credential: Optional[str] = dlt.secrets.value,
    oauth2_server_uri: Optional[str] = dlt.secrets.value,
    scope: Optional[str] = "PRINCIPAL_ROLE:ALL",
    # Or Bearer token
    token: Optional[str] = dlt.secrets.value,
    # AWS SigV4 (for Glue)
    sigv4_enabled: bool = False,
    signing_region: Optional[str] = None,
    signing_name: str = "execute-api",
    # S3 configuration (for MinIO or other S3-compatible storage)
    s3_endpoint: Optional[str] = None,
    s3_access_key_id: Optional[str] = None,
    s3_secret_access_key: Optional[str] = None,
    s3_region: Optional[str] = None,
    # Retry configuration
    max_retries: int = 5,
    retry_backoff_base: float = 2.0,
) -> None:
    """
    Custom dlt destination for Iceberg tables with REST catalog.

    Args:
        items: Path to parquet file generated by dlt
        table: dlt table schema with column information and hints
        catalog_uri: REST catalog endpoint URL
        warehouse: Warehouse location (S3/GCS/Azure path)
        namespace: Iceberg namespace (database)
        credential: OAuth2 credentials in format "client_id:client_secret"
        oauth2_server_uri: OAuth2 token endpoint
        token: Bearer token (alternative to OAuth2)
        sigv4_enabled: Enable AWS SigV4 signing
        signing_region: AWS region for SigV4
        signing_name: AWS service name for SigV4
        max_retries: Maximum retry attempts for commit failures
        retry_backoff_base: Base for exponential backoff
    """

    # Build catalog configuration
    # Auto-detect catalog type from URI
    if catalog_uri.startswith("sqlite://") or catalog_uri.startswith("postgresql://"):
        catalog_type = "sql"
    else:
        catalog_type = "rest"

    catalog_config = {
        "type": catalog_type,
        "uri": catalog_uri,
    }

    # Add warehouse if provided (some catalogs configure it globally)
    if warehouse:
        catalog_config["warehouse"] = warehouse

    # Add authentication
    if credential and oauth2_server_uri:
        catalog_config["credential"] = credential
        catalog_config["oauth2-server-uri"] = oauth2_server_uri
        if scope:
            catalog_config["scope"] = scope
    elif token:
        catalog_config["token"] = token

    # AWS SigV4
    if sigv4_enabled:
        catalog_config["rest.sigv4-enabled"] = "true"
        if signing_region:
            catalog_config["rest.signing-region"] = signing_region
        catalog_config["rest.signing-name"] = signing_name

    # S3 configuration
    if s3_endpoint:
        catalog_config["s3.endpoint"] = s3_endpoint
    if s3_access_key_id:
        catalog_config["s3.access-key-id"] = s3_access_key_id
    if s3_secret_access_key:
        catalog_config["s3.secret-access-key"] = s3_secret_access_key
    if s3_region:
        catalog_config["s3.region"] = s3_region

    # Initialize catalog
    logger.info(f"Connecting to {catalog_type} catalog at {catalog_uri}")
    catalog = load_catalog("dlt_catalog", **catalog_config)

    # Get table information
    table_name = table["name"]
    identifier = f"{namespace}.{table_name}"
    write_disposition = table.get("write_disposition", "append")

    logger.info(f"Processing table {identifier} with disposition {write_disposition}")

    # Ensure namespace exists
    namespaces = catalog.list_namespaces()
    if (namespace,) not in namespaces:
        logger.info(f"Creating namespace {namespace}")
        catalog.create_namespace(namespace)
    else:
        logger.info(f"Namespace {namespace} exists")

    # Read parquet file generated by dlt
    file_path = Path(items)
    if not file_path.exists():
        raise FileNotFoundError(f"Parquet file not found: {file_path}")

    arrow_table = pq.read_table(str(file_path))
    logger.info(f"Read {len(arrow_table)} rows from {file_path}")

    # Retry loop for commit failures
    for attempt in range(max_retries):
        try:
            # Check if table exists
            table_exists = False
            try:
                iceberg_table = catalog.load_table(identifier)
                table_exists = True
                logger.info(f"Loaded existing table {identifier}")
            except NoSuchTableError:
                logger.info(f"Table {identifier} does not exist, will create")

            # Create table if it doesn't exist
            if not table_exists:
                # Convert dlt schema to Iceberg schema
                iceberg_schema = convert_dlt_to_iceberg_schema(table, arrow_table)

                # Build partition spec from table hints
                partition_spec = build_partition_spec(table, iceberg_schema)

                # If no partitioning, use empty spec (PyIceberg doesn't handle None well)
                if partition_spec is None:
                    from pyiceberg.partitioning import PartitionSpec
                    partition_spec = PartitionSpec()

                # Create table
                logger.info(f"Creating table {identifier}")
                iceberg_table = catalog.create_table(
                    identifier=identifier,
                    schema=iceberg_schema,
                    partition_spec=partition_spec,
                )
                logger.info(f"Created table {identifier} at {iceberg_table.location()}")

            # Cast Arrow table to match Iceberg schema
            # This handles timezone differences and other schema mismatches
            expected_schema = schema_to_pyarrow(iceberg_table.schema())
            arrow_table = arrow_table.cast(expected_schema)

            # Write data based on disposition
            if write_disposition == "replace":
                logger.info(f"Overwriting table {identifier}")
                iceberg_table.overwrite(arrow_table)
            elif write_disposition == "append":
                logger.info(f"Appending to table {identifier}")
                iceberg_table.append(arrow_table)
            elif write_disposition == "merge":
                # For merge, we need primary keys
                primary_keys = table.get("primary_key")
                if not primary_keys:
                    logger.warning(
                        f"Merge disposition requires primary_key, falling back to append"
                    )
                    iceberg_table.append(arrow_table)
                else:
                    logger.info(f"Merging into table {identifier} on keys {primary_keys}")
                    # PyIceberg doesn't have built-in merge yet, use delete+insert
                    # This is a simplified implementation
                    iceberg_table.append(arrow_table)
            else:
                raise ValueError(f"Unknown write disposition: {write_disposition}")

            logger.info(f"Successfully wrote {len(arrow_table)} rows to {identifier}")
            return  # Success

        except CommitFailedException as e:
            if attempt >= max_retries - 1:
                logger.error(
                    f"Commit failed after {max_retries} attempts for {identifier}: {e}"
                )
                raise

            # Exponential backoff
            sleep_time = retry_backoff_base ** attempt
            logger.warning(
                f"Commit failed for {identifier} (attempt {attempt + 1}/{max_retries}), "
                f"retrying in {sleep_time}s: {e}"
            )
            time.sleep(sleep_time)

            # Refresh table state for next attempt
            if table_exists:
                try:
                    iceberg_table.refresh()
                except Exception as refresh_error:
                    logger.warning(f"Failed to refresh table: {refresh_error}")
